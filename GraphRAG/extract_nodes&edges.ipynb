{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from Web URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top Tech leaders\n",
    "url_list=[  \n",
    "    'https://en.wikipedia.org/wiki/Elon_Musk',\n",
    "    'https://en.wikipedia.org/wiki/Mark_Zuckerberg',\n",
    "    'https://en.wikipedia.org/wiki/Bill_Gates',\n",
    "    'https://en.wikipedia.org/wiki/Jeff_Bezos',\n",
    "    'https://en.wikipedia.org/wiki/Steve_Jobs',\n",
    "    'https://en.wikipedia.org/wiki/Sam_Altman',\n",
    "    'https://en.wikipedia.org/wiki/Larry_Ellison',\n",
    "    'https://en.wikipedia.org/wiki/Larry_Page',\n",
    "    'https://en.wikipedia.org/wiki/Sundar_Pichai',\n",
    "    'https://en.wikipedia.org/wiki/Satya_Nadella' \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean the extracted web URL data\n",
    "import re #for regular expression \n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# extract the data from the URLs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "def extract_data_from_URL(url):\n",
    "    loader=WebBaseLoader([url])\n",
    "    data=loader.load().pop().page_content\n",
    "    data=clean_text(data)\n",
    "    documents=[Document(page_content=data)]\n",
    "    # print(documents)\n",
    "    splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "    smaller_doc=splitter.split_documents(documents)\n",
    "    print(len(smaller_doc))\n",
    "    return smaller_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv \n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "import os  \n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm=AzureOpenAI(\n",
    "        api_key=os.getenv('GRAPHRAG_API_KEY'),\n",
    "        azure_endpoint='https://llmops-amruth.openai.azure.com/',\n",
    "        api_version=os.getenv('AZURE_OpenAI_API_VERSION')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create system prompt to extract data in JSON format as required\n",
    "\n",
    "system=\"\"\" You are a network graph maker tasked with analyzing the relationships involving top leaders in the world. Your job is to process the provided context chunk \n",
    "and extract an ontology of terms that represent key entrepreneurs, their associated entities, and all kinds of relationships present in the context.\n",
    "\n",
    "**Guidelines for Extraction:**\n",
    "\n",
    "1. **Identify Key Entrepreneurs and Related Terms**:\n",
    "   - Extract key entrepreneurs and related concepts such as:\n",
    "     - Companies, organizations, or industries they are associated with.\n",
    "     - Collaborators, partners, rivals, or competitors.\n",
    "     - Key innovations, achievements, or milestones.\n",
    "     - Locations, events, or time periods relevant to their actions.\n",
    "\n",
    "2. **Identify Relationships**:\n",
    "   - Extract all types of relationships between entrepreneurs and other entities (or between entities themselves).\n",
    "   - Relationships can include:\n",
    "     - Professional roles or associations.\n",
    "     - Business partnerships, collaborations, or rivalries.\n",
    "     - Innovations or contributions to industries.\n",
    "     - Personal connections or influences.\n",
    "     - Historical events or shared milestones.\n",
    "\n",
    "3. **Define Relationships**:\n",
    "   - Clearly specify the nature of each relationship in simple and concise terms.\n",
    "   - Relationships should convey meaningful connections relevant to the context.\n",
    "\n",
    "**Response Format**:\n",
    "- Provide your output **strictly as a list of JSON objects**. No additional text, descriptions,tags or comments are allowed.\n",
    "- Each object should include the following fields:\n",
    "  - `\"node_1\"`: The first entity in the relationship (can be a person, organization, or concept).\n",
    "  - `\"node_2\"`: The second entity in the relationship.\n",
    "  - `\"edge\"`: A concise sentence describing the relationship between `node_1` and `node_2`.\n",
    "\n",
    "**Example Output**:\n",
    "[\n",
    "   {\n",
    "       \"node_1\": \"Elon Musk\",\n",
    "       \"node_2\": \"SpaceX\",\n",
    "       \"edge\": \"Elon Musk founded SpaceX to revolutionize space exploration.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Steve Jobs\",\n",
    "       \"node_2\": \"Apple Inc.\",\n",
    "       \"edge\": \"Steve Jobs co-founded Apple Inc., a leading tech company.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Mark Zuckerberg\",\n",
    "       \"node_2\": \"Sheryl Sandberg\",\n",
    "       \"edge\": \"Sheryl Sandberg worked closely with Mark Zuckerberg as COO of Facebook.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Jeff Bezos\",\n",
    "       \"node_2\": \"Blue Origin\",\n",
    "       \"edge\": \"Jeff Bezos founded Blue Origin to focus on space exploration.\"\n",
    "   }\n",
    "]\n",
    "\n",
    "**Important Note**:\n",
    "- Always respond exclusively in JSON format. Any deviation from the JSON structure or inclusion of additional text will not be accepted.\n",
    "- Do not use code block formatting like ` ``` `.\n",
    "- Output must be a valid JSON array of objects without any surrounding text.\n",
    "\n",
    "Please provide the context containing information about entrepreneurs and their relationships for analysis.\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: the below code extracts the nodes and edges from the wikipedia links using LLM models\n",
    "1. cycle through the LLM models to extract the data (since gemini API is free it has a limit for each model)\n",
    "2. loop through the URL's to extract the data (extract_data_from_URL function we defined earlier)\n",
    "3. from the extracted data use the LLM to get the nodes and edges in JSON format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "102\n",
      "150\n",
      "133\n",
      "151\n",
      "41\n",
      "62\n",
      "85\n",
      "36\n",
      "32\n",
      "extracted information in 0:01:50.764209\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dotenv.load_dotenv()\n",
    "results=[]\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://llmops-amruth.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OpenAI_API_VERSION')\n",
    "#os.environ[\"OPENAI_API_KEY\"] = os.getenv('GRAPHRAG_API_KEY')\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('GRAPHRAG_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "start_time=datetime.now()\n",
    "for url in url_list:\n",
    "\n",
    "\n",
    "    \n",
    "    smaller_doc=extract_data_from_URL(url)\n",
    "    for doc in smaller_doc[:1]:\n",
    "        try:\n",
    "            chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": doc.page_content,\n",
    "                }\n",
    "            ],\n",
    "            model=os.getenv('AZURE_OPENAI_DEPLOYMENT_MODEL'),\n",
    "            )\n",
    "            results.append(chat_completion.choices[0].message.content)\n",
    "        \n",
    "        except Exception as e:\n",
    "        #print('Exception',e)\n",
    "            # errordata=e.args[0]\n",
    "            print(e)\n",
    "    \n",
    "            # if 'exhausted' in errordata:\n",
    "            #     print('Rate limit exceeded for model:', model_name)\n",
    "            #     model_name = next(model_cycle)  # Switch to the next model\n",
    "            #     print('Switching to model:', model_name)\n",
    "                \n",
    "end_time=datetime.now()\n",
    "len(results)\n",
    "print(f'extracted information in {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work with the json results from the LLM \n",
    "- next time we run the code we dont need the LLM to capture the nodes and edges again, so we store the json file\n",
    "- there are some cases when the LLM return a buggy JSON object there we exclude it\n",
    "- we store all the json objects to file Nodes_and_edges.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#print(results)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m combined_nodes_and_edges\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         combined_nodes_and_edges\u001b[38;5;241m.\u001b[39mextend(json\u001b[38;5;241m.\u001b[39mloads(res)) \u001b[38;5;66;03m#convert the string result from LLM to JSON \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import json \n",
    "#print(results)\n",
    "combined_nodes_and_edges=[]\n",
    "for res in results:\n",
    "    try:\n",
    "        combined_nodes_and_edges.extend(json.loads(res)) #convert the string result from LLM to JSON \n",
    "    except Exception as e:\n",
    "        print('buggy JSON object', e)\n",
    "\n",
    "with open('Nodes_and_edges.json','w') as file:\n",
    "    json.dump(combined_nodes_and_edges,file,indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gephistreamer import graph\n",
    "from gephistreamer import streamer\n",
    "import json \n",
    "# connect to gephi server\n",
    "# create a stream \n",
    "stream = streamer.Streamer(streamer.GephiWS(hostname=\"localhost\", port=8080, workspace=\"workspace1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Nodes_and_edges.json','r') as file:\n",
    "    results=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop throgugh the list of json i.e. results \n",
    "for res in results:\n",
    "    try:        \n",
    "        node_a = graph.Node(res['node_1'],custom_property=1)\n",
    "        node_b = graph.Node(res['node_2'],custom_property=2)\n",
    "        stream.add_node(node_a,node_b)\n",
    "        edge_ab = graph.Edge(node_a,node_b,custom_property=res['edge'])\n",
    "        stream.add_edge(edge_ab)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('buggy JSON object', e,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
